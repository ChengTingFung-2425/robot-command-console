# TUI + LLM Integration Lessons

æ­¤æ–‡ä»¶åŒ…å« TUI èˆ‡ LLM è‡ªç„¶èªè¨€æ§åˆ¶æ•´åˆçš„è©³ç´°ç¶“é©—æ•™è¨“ã€‚

åŒ…å«ï¼šTextual æ¡†æ¶ã€LLM æ•´åˆã€è‡ªç„¶èªè¨€è™•ç†ç­‰ç¶“é©—ã€‚

## ğŸ“š ç›¸é—œæ–‡ä»¶

- **[â† è¿”å›ä¸»è¨˜æ†¶](../PROJECT_MEMORY.md)** - Top 15 é—œéµç¶“é©—
- **[Phase 3 ç¶“é©—](phase3_lessons.md)** - æœå‹™æ•´åˆ
- **[å®‰å…¨æ€§](security_lessons.md)** - æç¤ºæ³¨å…¥é˜²è­·ã€è¼¸å…¥é©—è­‰
- **[ä»£ç¢¼å“è³ª](code_quality_lessons.md)** - æ¸¬è©¦ç­–ç•¥

---

## ğŸ¨ TUI + LLM è‡ªç„¶èªè¨€æ§åˆ¶æ•´åˆï¼ˆ2025-12-11ï¼‰

> ğŸ“– **è©³ç´°æŒ‡å—**ï¼š[development/TUI_LLM_INTEGRATION_GUIDE.md](development/TUI_LLM_INTEGRATION_GUIDE.md)

### åŠŸèƒ½å¯¦ä½œç¸½çµ

**ç›®æ¨™**ï¼šå»ºç«‹å®Œæ•´çš„ TUI èˆ‡ LLM è‡ªç„¶èªè¨€æ§åˆ¶ç³»çµ±ï¼Œå¯¦ç¾ã€Œäººé¡è‡ªç„¶èªè¨€ â†’ LLM ç†è§£ â†’ çœŸå¯¦æ©Ÿå™¨äººåŸ·è¡Œã€çš„å®Œæ•´æµç¨‹ã€‚

**å¯¦ä½œæ¨¡çµ„**ï¼š
1. **TUI æ ¸å¿ƒ**ï¼ˆ`src/robot_service/tui/`ï¼‰- Textual çµ‚ç«¯ä»‹é¢
2. **LLM IPC Bridge**ï¼ˆ`src/llm_discovery/bridge.py`ï¼‰- çœŸå¯¦ HTTP æ©‹æ¨‘
3. **LLM Command Processor**ï¼ˆ`src/robot_service/llm_command_processor.py`ï¼‰- è‡ªç„¶èªè¨€è™•ç†
4. **Robot Action Consumer**ï¼ˆ`src/robot_service/robot_action_consumer.py`ï¼‰- çœŸå¯¦æ©Ÿå™¨äººæ•´åˆ
5. **LLM Trace Manager**ï¼ˆ`src/robot_service/llm_trace_manager.py`ï¼‰- å®Œæ•´è¿½è¹¤ç³»çµ±
6. **WebUI GUI**ï¼ˆ`WebUI/templates/llm_*.html`ï¼‰- åœ–å½¢åŒ–ä»‹é¢

**æ¸¬è©¦è¦†è“‹**ï¼š37 å€‹ TUI æ¸¬è©¦ï¼Œ100% é€šé

---

### 12.1 çœŸå¯¦ HTTP/IPC å‘¼å«å¯¦ä½œ

```python
# âœ… ä½¿ç”¨ aiohttp å¯¦ä½œçœŸå¯¦ HTTP è«‹æ±‚
async with aiohttp.ClientSession() as session:
    response = await session.post(
        f"{endpoint}/invoke/{skill_id}",
        json={"jsonrpc": "2.0", "method": "invoke", "params": parameters},
        timeout=aiohttp.ClientTimeout(total=30)
    )
    result = await response.json()
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **é€£æ¥æ± ç®¡ç†**ï¼šé‡ç”¨ HTTP æœƒè©±ä»¥æå‡æ•ˆèƒ½
2. **è¶…æ™‚æ§åˆ¶**ï¼šæ‰€æœ‰è«‹æ±‚éƒ½æ‡‰è¨­å®šè¶…æ™‚ï¼ˆé è¨­ 30 ç§’ï¼‰
3. **è‡ªå‹•é‡è©¦**ï¼š5xx éŒ¯èª¤è‡ªå‹•é‡è©¦ï¼ˆæŒ‡æ•¸é€€é¿ï¼Œæœ€å¤š 3 æ¬¡ï¼‰
4. **éŒ¯èª¤éš”é›¢**ï¼šHTTP éŒ¯èª¤ä¸æ‡‰å½±éŸ¿ä¸»ç¨‹å¼

---

### 12.2 LLM è‡ªç„¶èªè¨€è™•ç†æµç¨‹

```python
# âœ… å®Œæ•´çš„ LLM è™•ç†æµç¨‹
async def process_text_command(self, text: str) -> Dict:
    # 1. å»ºç«‹è¿½è¹¤
    trace_id = self.trace_manager.start_trace()
    
    # 2. è¨˜éŒ„è¼¸å…¥
    self.trace_manager.log_event(trace_id, INPUT_RECEIVED, {"text": text})
    
    # 3. å‘¼å« LLM
    response = await self.llm_provider.chat_completion(
        messages=self.conversation_history + [{"role": "user", "content": text}],
        functions=self.available_functions
    )
    
    # 4. åŸ·è¡Œ function call
    if response.get("function_call"):
        result = await self.bridge.call_from_llm(response["function_call"])
    
    # 5. æ›´æ–°å°è©±æ­·å²
    self.conversation_history.append(...)
    
    return {"trace_id": trace_id, "result": result}
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **å°è©±æ­·å²**ï¼šç¶­è­·æœ€è¿‘ 10 æ¢å°è©±ä»¥æä¾›ä¸Šä¸‹æ–‡
2. **è¿½è¹¤è²«ç©¿**ï¼šæ¯å€‹æ­¥é©Ÿéƒ½è¨˜éŒ„è¿½è¹¤äº‹ä»¶
3. **éŒ¯èª¤è™•ç†**ï¼šLLM API å¤±æ•—æ‡‰æœ‰å‹å–„æç¤º
4. **è¶…æ™‚ä¿è­·**ï¼šLLM è«‹æ±‚å¯èƒ½å¾ˆæ…¢ï¼Œéœ€è¨­å®šåˆç†è¶…æ™‚

---

### 12.3 è¿½è¹¤ç³»çµ±è¨­è¨ˆæ¨¡å¼

```python
# âœ… 10 ç¨®è¿½è¹¤äº‹ä»¶æ¶µè“‹å®Œæ•´æµç¨‹
INPUT_RECEIVED    â†’ æ”¶åˆ°è¼¸å…¥
LLM_REQUEST       â†’ LLM è«‹æ±‚
LLM_RESPONSE      â†’ LLM å›æ‡‰
FUNCTION_CALL     â†’ Function call
BRIDGE_CALL       â†’ Bridge HTTP å‘¼å«
FUNCTION_EXECUTED â†’ Function åŸ·è¡Œå®Œæˆ
QUEUE_ENQUEUED    â†’ åŠ å…¥ä½‡åˆ—
ROBOT_EXECUTED    â†’ æ©Ÿå™¨äººåŸ·è¡Œ
ERROR             â†’ éŒ¯èª¤
COMPLETED         â†’ å®Œæˆ
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **å®Œæ•´æ€§**ï¼šè¿½è¹¤æ¯å€‹é—œéµæ­¥é©Ÿ
2. **æ™‚é–“æˆ³**ï¼šè¨˜éŒ„æ¯å€‹äº‹ä»¶çš„æ™‚é–“
3. **æŒçºŒæ™‚é–“**ï¼šè¨ˆç®—æ¯å€‹éšæ®µçš„è€—æ™‚
4. **è¨‚é–±æ©Ÿåˆ¶**ï¼šæ”¯æ´å³æ™‚äº‹ä»¶è¨‚é–±

---

### 12.4 çœŸå¯¦æ©Ÿå™¨äººæ•´åˆæ¶æ§‹

```python
# âœ… å¾ä½‡åˆ—æ¶ˆè²»å‹•ä½œä¸¦ç™¼é€çµ¦çœŸå¯¦æ©Ÿå™¨äºº
class RobotActionConsumer:
    async def start(self):
        while self._running:
            # 1. å¾ä½‡åˆ—è®€å–
            action = await self.service_manager.dequeue()
            
            # 2. è½‰æ›æ ¼å¼
            robot_command = self.translate_action(action)
            
            # 3. ç™¼é€çµ¦çœŸå¯¦æ©Ÿå™¨äºº
            result = await self.robot_connector.send_command(
                robot_id=action["robot_id"],
                command=robot_command
            )
            
            # 4. å›å ±çµæœ
            await self.report_result(action["trace_id"], result)
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **è§£è€¦åˆ**ï¼šä½‡åˆ—ä½œç‚ºç·©è¡ï¼Œè§£è€¦ LLM å’Œæ©Ÿå™¨äºº
2. **æ ¼å¼è½‰æ›**ï¼šçµ±ä¸€çš„å…§éƒ¨æ ¼å¼ â†’ æ©Ÿå™¨äººç‰¹å®šæ ¼å¼
3. **éŒ¯èª¤æ¢å¾©**ï¼šæ©Ÿå™¨äººé€£æ¥å¤±æ•—æ‡‰é‡è©¦æˆ–é€šçŸ¥
4. **çµæœå›å ±**ï¼šåŸ·è¡Œçµæœæ‡‰å›å‚³çµ¦è¿½è¹¤ç³»çµ±

---

### 12.5 WebUI GUI è¨­è¨ˆæ¨¡å¼

```javascript
// âœ… WebSocket å³æ™‚æ›´æ–°
const ws = new WebSocket('ws://localhost:5000/ws/llm');

ws.onmessage = (event) => {
    const data = JSON.parse(event.data);
    
    if (data.type === 'trace_event') {
        updateTraceView(data.event);
    }
    
    if (data.type === 'llm_response') {
        appendMessage(data.message);
    }
};
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **å³æ™‚æ€§**ï¼šä½¿ç”¨ WebSocket è€Œéè¼ªè©¢
2. **éŸ¿æ‡‰å¼**ï¼šæ”¯æ´æ¡Œé¢å’Œè¡Œå‹•è£ç½®
3. **éŒ¯èª¤è™•ç†**ï¼šç¶²è·¯æ–·ç·šæ‡‰æœ‰æ˜ç¢ºæç¤º
4. **ä½¿ç”¨è€…é«”é©—**ï¼šLoading ç‹€æ…‹ã€Toast é€šçŸ¥

---

### 12.6 JSON-RPC 2.0 å”å®šæ¨™æº–åŒ–

```python
# âœ… æ¨™æº– JSON-RPC 2.0 æ ¼å¼
request = {
    "jsonrpc": "2.0",
    "method": "invoke",
    "params": {
        "robot_id": "robot-001",
        "action": "go_forward"
    },
    "id": "req-123"
}

response = {
    "jsonrpc": "2.0",
    "result": {"status": "success", "data": ...},
    "id": "req-123"
}

# éŒ¯èª¤å›æ‡‰
error_response = {
    "jsonrpc": "2.0",
    "error": {
        "code": -32600,
        "message": "Invalid Request"
    },
    "id": "req-123"
}
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **æ¨™æº–åŒ–**ï¼šä½¿ç”¨ JSON-RPC 2.0 æå‡ç›¸å®¹æ€§
2. **éŒ¯èª¤ä»£ç¢¼**ï¼šæ¨™æº–éŒ¯èª¤ä»£ç¢¼ï¼ˆ-32xxxï¼‰
3. **è«‹æ±‚ ID**ï¼šæ”¯æ´è«‹æ±‚-å›æ‡‰å°æ‡‰
4. **æ‰¹æ¬¡è«‹æ±‚**ï¼šå¯æ“´å±•æ”¯æ´æ‰¹æ¬¡æ“ä½œ

---

### 12.7 OpenAI Function Calling æ•´åˆ

```python
# âœ… å®šç¾© functions ä¾› LLM ä½¿ç”¨
functions = [
    {
        "name": "robot_command",
        "description": "Send command to robot",
        "parameters": {
            "type": "object",
            "properties": {
                "robot_id": {"type": "string", "description": "Robot ID"},
                "action": {"type": "string", "description": "Action name"},
                "params": {"type": "object", "description": "Action parameters"}
            },
            "required": ["robot_id", "action"]
        }
    }
]

# LLM æœƒè¿”å› function_call
function_call = {
    "name": "robot_command",
    "arguments": '{"robot_id": "robot-001", "action": "go_forward", "params": {"duration_ms": 3000}}'
}
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **æè¿°æ¸…æ™°**ï¼šfunction å’Œåƒæ•¸çš„æè¿°è¦æ¸…æ¥š
2. **JSON Schema**ï¼šä½¿ç”¨æ¨™æº– schema é©—è­‰åƒæ•¸
3. **éŒ¯èª¤è™•ç†**ï¼šåƒæ•¸è§£æå¤±æ•—æ‡‰æœ‰å‹å–„æç¤º
4. **å¤š LLM æ”¯æ´**ï¼šOpenAI/Claude/Local æ ¼å¼ç•¥æœ‰ä¸åŒ

---

### 12.8 TUI éåŒæ­¥äº‹ä»¶è™•ç†

```python
# âœ… Textual éåŒæ­¥äº‹ä»¶è™•ç†
class RobotConsoleTUI(App):
    async def on_mount(self):
        # å•Ÿå‹•èƒŒæ™¯ä»»å‹™
        self.update_task = asyncio.create_task(self._update_status())
    
    async def _update_status(self):
        while True:
            try:
                await asyncio.sleep(1)
                # æ›´æ–°æœå‹™ç‹€æ…‹
                await self.refresh_services()
            except asyncio.CancelledError:
                # ä»»å‹™å–æ¶ˆæ™‚å±¬é æœŸè¡Œç‚ºï¼Œå®‰å…¨å¿½ç•¥
                break
    
    async def on_unmount(self):
        # æ¸…ç†ä»»å‹™
        if self.update_task:
            self.update_task.cancel()
            try:
                await self.update_task
            except asyncio.CancelledError:
                pass
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **ç”Ÿå‘½é€±æœŸ**ï¼šæ­£ç¢ºè™•ç† mount/unmount
2. **ä»»å‹™å–æ¶ˆ**ï¼šå„ªé›…è™•ç† CancelledError
3. **ç•°å¸¸éš”é›¢**ï¼šèƒŒæ™¯ä»»å‹™éŒ¯èª¤ä¸æ‡‰å´©æ½° UI
4. **è³‡æºæ¸…ç†**ï¼šç¢ºä¿æ‰€æœ‰ä»»å‹™æ­£ç¢ºé—œé–‰

---

### 12.9 æŒ‡ä»¤è§£ææ¨¡å¼è¨­è¨ˆ

```python
# âœ… æ¸…æ™°çš„æŒ‡ä»¤è§£æé‚è¼¯
def parse_command(command: str) -> Tuple[str, str]:
    """
    è§£ææŒ‡ä»¤é¡å‹å’Œåƒæ•¸
    
    æ”¯æ´æ ¼å¼ï¼š
    - "action" â†’ ("robot", "action")
    - "robot-id:action" â†’ ("robot", "action")
    - "all:action" â†’ ("broadcast", "action")
    - "system:action" â†’ ("system", "action")
    - "service:name.action" â†’ ("service", "name.action")
    - "llm:on/off" â†’ ("llm", "on/off")
    - "trace:id" â†’ ("trace", "id")
    """
    if ':' not in command:
        return ("robot", command)
    
    prefix, suffix = command.split(':', 1)
    
    if prefix in ["system", "service", "llm", "trace"]:
        return (prefix, suffix)
    elif prefix == "all":
        return ("broadcast", suffix)
    else:
        return ("robot", f"{prefix}:{suffix}")
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **çµ±ä¸€æ ¼å¼**ï¼šæ‰€æœ‰æŒ‡ä»¤éµå¾ªç›¸åŒæ ¼å¼
2. **å‘å¾Œç›¸å®¹**ï¼šç„¡å‰ç¶´æŒ‡ä»¤é è¨­ç‚ºæ©Ÿå™¨äººæŒ‡ä»¤
3. **æ¸…æ™°æ–‡ä»¶**ï¼šdocstring èªªæ˜æ‰€æœ‰æ”¯æ´æ ¼å¼
4. **éŒ¯èª¤è™•ç†**ï¼šç„¡æ•ˆæ ¼å¼æ‡‰æœ‰å‹å–„æç¤º

---

### 12.10 å¤š LLM æä¾›å•†æŠ½è±¡

```python
# âœ… æä¾›å•†æŠ½è±¡ä»‹é¢
class LLMProviderBase(ABC):
    @abstractmethod
    async def chat_completion(
        self,
        messages: List[Dict],
        functions: Optional[List[Dict]] = None
    ) -> Dict:
        pass

# OpenAI å¯¦ä½œ
class OpenAIProvider(LLMProviderBase):
    async def chat_completion(self, messages, functions=None):
        response = await openai.ChatCompletion.acreate(
            model="gpt-4",
            messages=messages,
            functions=functions
        )
        return response

# Claude å¯¦ä½œï¼ˆå¾…å¯¦ä½œï¼‰
class ClaudeProvider(LLMProviderBase):
    async def chat_completion(self, messages, functions=None):
        # è½‰æ›ç‚º Claude æ ¼å¼
        # å‘¼å« Anthropic API
        pass

# æœ¬åœ° LLM å¯¦ä½œï¼ˆå¾…å¯¦ä½œï¼‰
class LocalLLMProvider(LLMProviderBase):
    async def chat_completion(self, messages, functions=None):
        # å‘¼å« Ollama/LM Studio
        pass
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **æŠ½è±¡ä»‹é¢**ï¼šå®šç¾©çµ±ä¸€çš„ä»‹é¢
2. **æ ¼å¼è½‰æ›**ï¼šæ¯å€‹æä¾›å•†è² è²¬æ ¼å¼è½‰æ›
3. **éŒ¯èª¤çµ±ä¸€**ï¼šå°‡ä¸åŒéŒ¯èª¤è½‰æ›ç‚ºçµ±ä¸€æ ¼å¼
4. **æ˜“æ“´å±•**ï¼šæ–°å¢æä¾›å•†åªéœ€å¯¦ä½œä»‹é¢

---

### 12.11 è¨˜æ†¶é«”ç®¡ç†èˆ‡æ¸…ç†

```python
# âœ… LRU å¿«å–é˜²æ­¢è¨˜æ†¶é«”æ´©æ¼
class LLMTraceManager:
    def __init__(self, max_traces: int = 1000):
        self._traces: OrderedDict[str, List[TraceEvent]] = OrderedDict()
        self.max_traces = max_traces
    
    def add_event(self, trace_id: str, event: TraceEvent):
        # è¶…éé™åˆ¶æ™‚ç§»é™¤æœ€èˆŠçš„è¿½è¹¤
        if len(self._traces) >= self.max_traces:
            self._traces.popitem(last=False)
        
        if trace_id not in self._traces:
            self._traces[trace_id] = []
        
        self._traces[trace_id].append(event)
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **å¤§å°é™åˆ¶**ï¼šè¨­å®šæœ€å¤§è¿½è¹¤æ•¸é‡
2. **LRU ç­–ç•¥**ï¼šè‡ªå‹•ç§»é™¤æœ€èˆŠçš„è¨˜éŒ„
3. **å®šæœŸæ¸…ç†**ï¼šæä¾›æ‰‹å‹•æ¸…ç†æ–¹æ³•
4. **ç›£æ§**ï¼šè¨˜éŒ„ç•¶å‰ä½¿ç”¨é‡

---

### 12.12 API éŒ¯èª¤è™•ç†æœ€ä½³å¯¦è¸

```python
# âœ… çµ±ä¸€çš„ API éŒ¯èª¤è™•ç†
@bp.route('/api/llm/chat', methods=['POST'])
async def chat():
    try:
        # 1. åƒæ•¸é©—è­‰
        data = request.get_json()
        if not data or 'message' not in data:
            return jsonify({
                'status': 'error',
                'error': {
                    'code': 'INVALID_PARAMETER',
                    'message': 'Missing required field: message'
                }
            }), 400
        
        # 2. æ¥­å‹™é‚è¼¯
        result = await process_message(data['message'])
        
        # 3. æˆåŠŸå›æ‡‰
        return jsonify({
            'status': 'success',
            'data': result
        })
    
    except Exception as e:
        # 4. è¨˜éŒ„è©³ç´°éŒ¯èª¤
        logger.error(f"Error in chat API: {e}", exc_info=True)
        
        # 5. å›å‚³é€šç”¨éŒ¯èª¤ï¼ˆä¸æš´éœ²å…§éƒ¨è³‡è¨Šï¼‰
        return jsonify({
            'status': 'error',
            'error': {
                'code': 'INTERNAL_ERROR',
                'message': 'An internal error has occurred.'
            }
        }), 500
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **åƒæ•¸é©—è­‰åœ¨å‰**ï¼šå…ˆé©—è­‰å†è™•ç†
2. **4xx vs 5xx**ï¼šå®¢æˆ¶ç«¯éŒ¯èª¤ vs ä¼ºæœå™¨éŒ¯èª¤
3. **è©³ç´°æ—¥èªŒ**ï¼šä½¿ç”¨ `exc_info=True` è¨˜éŒ„å †ç–Š
4. **é€šç”¨éŒ¯èª¤è¨Šæ¯**ï¼šä¸æš´éœ²å…§éƒ¨å¯¦ä½œç´°ç¯€
5. **çµ±ä¸€æ ¼å¼**ï¼šæ‰€æœ‰ API ä½¿ç”¨ç›¸åŒçš„éŒ¯èª¤æ ¼å¼

---

### 12.13 WebSocket ç”Ÿå‘½é€±æœŸç®¡ç†

```python
# âœ… WebSocket é€£æ¥ç®¡ç†
class LLMWebSocketHandler:
    def __init__(self):
        self.connections: Set[WebSocket] = set()
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.connections.add(websocket)
    
    async def disconnect(self, websocket: WebSocket):
        self.connections.discard(websocket)
    
    async def broadcast(self, message: Dict):
        # ç§»é™¤å·²æ–·ç·šçš„é€£æ¥
        dead_connections = set()
        
        for connection in self.connections:
            try:
                await connection.send_json(message)
            except Exception:
                dead_connections.add(connection)
        
        # æ¸…ç†æ–·ç·šé€£æ¥
        self.connections -= dead_connections
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **é€£æ¥è¿½è¹¤**ï¼šç¶­è­·æ´»èºé€£æ¥é›†åˆ
2. **æ–·ç·šè™•ç†**ï¼šbroadcast æ™‚æª¢æ¸¬ä¸¦æ¸…ç†æ–·ç·š
3. **å„ªé›…é—œé–‰**ï¼šdisconnect æ™‚æ­£ç¢ºç§»é™¤
4. **éŒ¯èª¤éš”é›¢**ï¼šå–®ä¸€é€£æ¥éŒ¯èª¤ä¸å½±éŸ¿å…¶ä»–

---

### 12.14 ç’°å¢ƒè®Šæ•¸é…ç½®ç®¡ç†

```python
# âœ… ä½¿ç”¨ç’°å¢ƒè®Šæ•¸é…ç½®
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
OLLAMA_ENDPOINT = os.environ.get("OLLAMA_ENDPOINT", "http://127.0.0.1:11434")
MCP_ENDPOINT = os.environ.get("MCP_ENDPOINT", "http://127.0.0.1:9001")

# âœ… é…ç½®é©—è­‰
if not OPENAI_API_KEY:
    logger.warning("OPENAI_API_KEY not set, OpenAI provider will not be available")
```

**ç¶“é©—æ•™è¨“**ï¼š
1. **ç’°å¢ƒè®Šæ•¸**ï¼šæ•æ„Ÿè³‡è¨Šä¸å¯«å…¥ç¨‹å¼ç¢¼
2. **åˆç†é è¨­å€¼**ï¼šæœ¬åœ°é–‹ç™¼å¸¸ç”¨å€¼ä½œç‚ºé è¨­
3. **é…ç½®é©—è­‰**ï¼šå•Ÿå‹•æ™‚æª¢æŸ¥å¿…è¦é…ç½®
4. **æ–‡ä»¶è¨˜éŒ„**ï¼šåœ¨ README åˆ—å‡ºæ‰€æœ‰ç’°å¢ƒè®Šæ•¸

---

### 12.15 ç¨‹å¼ç¢¼å¯©æŸ¥è‡ªå‹•åŒ–å›é¥‹

**æœ¬æ¬¡ PR å¯¦ä½œè¦æ¨¡**ï¼š
- **æ–°å¢æª”æ¡ˆ**ï¼š11 å€‹ï¼ˆ~4500 è¡Œï¼‰
- **ä¿®æ”¹æª”æ¡ˆ**ï¼š3 å€‹ï¼ˆ~150 è¡Œï¼‰
- **æ¸¬è©¦**ï¼š37 å€‹ï¼ˆ100% é€šéï¼‰
- **æ–‡ä»¶**ï¼š5 å€‹ï¼ˆ~2000 è¡Œï¼‰

**Code Review ç™¼ç¾**ï¼š
- ç„¡é‡å¤§å•é¡Œ
- å»ºè­°åŠ å¼·éŒ¯èª¤è™•ç†æ¸¬è©¦ï¼ˆå·²å®Œæˆï¼‰
- å»ºè­°å®Œå–„æ–‡ä»¶ï¼ˆå·²å®Œæˆï¼‰

**ç¶“é©—æ•™è¨“**ï¼š
1. **æ—©æœŸ Review**ï¼šåœ¨å¯¦ä½œéç¨‹ä¸­æŒçºŒ review
2. **è‡ªå‹•åŒ–å·¥å…·**ï¼šflake8ã€mypy è‡ªå‹•æª¢æŸ¥
3. **æ–‡ä»¶åŒæ­¥**ï¼šç¨‹å¼ç¢¼èˆ‡æ–‡ä»¶åŒæ­¥æ›´æ–°
4. **æ¸¬è©¦è¦†è“‹**ï¼šTDD æ–¹å¼ç¢ºä¿æ¸¬è©¦å®Œæ•´

---

